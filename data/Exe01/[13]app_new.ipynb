{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-03T10:59:16.683898Z",
     "start_time": "2019-01-03T10:59:15.361004Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "import seaborn as sns\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-12T11:54:55.702669Z",
     "start_time": "2018-12-12T11:54:55.695688Z"
    }
   },
   "source": [
    "# 1. Mnist 영상 이해하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-03T10:59:20.853920Z",
     "start_time": "2019-01-03T10:59:20.681379Z"
    }
   },
   "outputs": [],
   "source": [
    "#1\n",
    "mnist = input_data.read_data_sets(\"mnist/\", one_hot=True)\n",
    "fashion = input_data.read_data_sets(\"fashion/\", source_url='http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/', one_hot=True)\n",
    "\n",
    "\n",
    "print(mnist.train.images.shape)\n",
    "print(mnist.validation.images.shape)\n",
    "print(mnist.test.images.shape)\n",
    "\n",
    "print(mnist.train.labels.shape)\n",
    "\n",
    "\n",
    "batch, batchy = mnist.train.next_batch(64)\n",
    "print(batch.shape)\n",
    "print(batchy.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-03T10:59:24.038628Z",
     "start_time": "2019-01-03T10:59:22.929087Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "for i in range(25):\n",
    "    plt.subplot(5,5,i+1)    \n",
    "    tmp = mnist.train.images[i].reshape(28,28)\n",
    "    plt.imshow(tmp, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3\n",
    "labels =  np.argmax([[ 0 ,0, 0, 1, 0, 0] , [ 0, 1, 0, 0, 0, 0 ] ], axis=1)\n",
    "#  [ 0 0 0 1 0 0]   =>     3   \n",
    "#  [ 0 1 0 0 0 0 ]  =>     1\n",
    "print(labels)\n",
    "labels = np.array([0,0,1,3,3,3,2,3])\n",
    "print(np.where(labels == 3   ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-03T10:59:27.952335Z",
     "start_time": "2019-01-03T10:59:26.104831Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#4\n",
    "labels =  np.argmax( mnist.train.labels, axis=1)\n",
    "idx = np.where(labels == 9)   \n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "for i in range(50):\n",
    "    plt.subplot(5,10,i+1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    tmp = mnist.train.images[idx[0][i], :].reshape(28,28)\n",
    "    plt.imshow(tmp, cmap='gray')     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-09T14:22:13.320974Z",
     "start_time": "2018-12-09T14:22:13.316995Z"
    }
   },
   "source": [
    "# 2. Fashion 영상 이해하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-03T10:59:30.661675Z",
     "start_time": "2019-01-03T10:59:30.505021Z"
    }
   },
   "outputs": [],
   "source": [
    "#1\n",
    "print(fashion.train.images.shape)\n",
    "print(fashion.validation.images.shape)\n",
    "print(fashion.test.images.shape)\n",
    "print(fashion.train.labels.shape)\n",
    "\n",
    "\n",
    "batch, batchy = fashion.train.next_batch(64)\n",
    "print(batch.shape)\n",
    "print(batchy.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-03T10:59:33.911963Z",
     "start_time": "2019-01-03T10:59:32.792419Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#2\n",
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', \n",
    "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "for i in range(25):\n",
    "    plt.subplot(5,5,i+1)\n",
    "    plt.grid(False)\n",
    "    tmp = fashion.train.images[i].reshape(28,28)\n",
    "    plt.imshow(tmp, cmap='gray') \n",
    "    plt.xlabel(class_names[ np.argmax(fashion.train.labels[i])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-03T10:59:38.090151Z",
     "start_time": "2019-01-03T10:59:36.137829Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#3\n",
    "labels =  np.argmax( fashion.train.labels, axis=1)\n",
    "idx = np.where(labels == 3)   \n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "for i in range(50):\n",
    "    plt.subplot(5,10,i+1)\n",
    "    tmp = fashion.train.images[idx[0][i], :].reshape(28,28)\n",
    "    plt.imshow(tmp, cmap='gray')     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  3. 미니 배치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1\n",
    "dim = 784\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, dim])\n",
    "y = tf.placeholder(tf.int32, [None, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2\n",
    "def addLayerReLU(x, node) :    \n",
    "    input = x.get_shape().as_list()    \n",
    "    initializer=tf.contrib.layers.variance_scaling_initializer()    \n",
    "    w = tf.Variable(initializer([input[1], node]))\n",
    "    b = tf.Variable(tf.random_normal([node]))\n",
    "    layer = tf.nn.relu(tf.matmul(x, w) + b)    \n",
    "    return layer\n",
    "\n",
    "def addLayer0(x, node) :    \n",
    "    input = x.get_shape().as_list()    \n",
    "    initializer = tf.contrib.layers.xavier_initializer()\n",
    "    w = tf.Variable(initializer([input[1], node]))\n",
    "    b = tf.Variable(tf.random_normal([node]))\n",
    "    layer = tf.matmul(x, w) + b\n",
    "    return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3    \n",
    "layer_1 = addLayerReLU(x, 256)\n",
    "layer_2 = addLayerReLU(layer_1, 256)\n",
    "hypothesis = addLayer0(layer_2, 10)\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=hypothesis, labels=y))\n",
    "\n",
    "train = tf.train.AdamOptimizer(learning_rate=0.001).minimize(cost)\n",
    "\n",
    "\"\"\"  \n",
    "  softmax(hypothesis) ->   [ 0.1  0.1   0.8]   -> argmax ->  [ 2 1]   \n",
    "                           [ 0,1  0.6   0.3] \n",
    "                           \n",
    "  y =  [ 0 0 1]     argmax ->   [ 2 0]\n",
    "       [ 1 0 0]\n",
    "  \n",
    "  equal(  [2 1] ,   [2  0] )    ->  [true, false]   ->  cast ->   [1.0  0.0]   -> mean   -> 0.5\n",
    "  \n",
    "  \n",
    "\"\"\" \n",
    "predicted = tf.argmax(tf.nn.softmax(hypothesis), 1)\n",
    "compared = tf.equal(predicted , tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(compared, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-30T11:50:47.200532Z",
     "start_time": "2018-12-30T11:50:20.317643Z"
    }
   },
   "outputs": [],
   "source": [
    "#4\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "\n",
    "batch_size = 50  \n",
    "total_batch = int(mnist.train.num_examples / batch_size)\n",
    "\n",
    "ts = []\n",
    "cs = []\n",
    "for epoch in range(30):\n",
    "    total_cost = 0\n",
    "    start_time = time.time()\n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)    \n",
    "        c, _ = sess.run([cost,train], feed_dict={x: batch_xs, y: batch_ys})                \n",
    "        total_cost += c\n",
    "    end_time = time.time()\n",
    "    ts.append(end_time - start_time)\n",
    "    cs.append(total_cost)\n",
    "    print(epoch, total_cost, end_time - start_time)\n",
    "        \n",
    "a = sess.run(accuracy,  feed_dict={x: mnist.test.images, y: mnist.test.labels})\n",
    "\n",
    "print(\"배치크기:{}   loop:{}    시간:{}      정확도:{}\".format(batch_size,total_batch, sum(ts),a))\n",
    "plt.plot(cs)\n",
    "plt.title(\"batch:{}  loop:{}   time:{:.2f}   cost:{:.5f}  acc:{:.4f}\".format(batch_size,total_batch, sum(ts), total_cost, a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  4.  drop out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1\n",
    "x = tf.placeholder(tf.float32, [None, dim])\n",
    "y = tf.placeholder(tf.int32, [None, 10])\n",
    "\n",
    "keep_prob = tf.placeholder(tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2\n",
    "def addLayerReLU(x, node, keep_prob) :    \n",
    "    input = x.get_shape().as_list()    \n",
    "    initializer=tf.contrib.layers.variance_scaling_initializer()    \n",
    "    w = tf.Variable(initializer([input[1], node]))\n",
    "    b = tf.Variable(tf.random_normal([node]))\n",
    "    layer = tf.nn.relu(tf.matmul(x, w) + b)    \n",
    "    layer = tf.nn.dropout(layer, keep_prob)  \n",
    "    return layer\n",
    "\n",
    "def addLayer0(x, node) :    \n",
    "    input = x.get_shape().as_list()    \n",
    "    initializer = tf.contrib.layers.xavier_initializer()\n",
    "    w = tf.Variable(initializer([input[1], node]))\n",
    "    b = tf.Variable(tf.random_normal([node]))\n",
    "    layer = tf.matmul(x, w) + b\n",
    "    return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3\n",
    "layer_1 = addLayerReLU(x, 256, keep_prob)\n",
    "layer_2 = addLayerReLU(layer_1, 256, keep_prob)\n",
    "hypothesis = addLayer0(layer_2, 10)\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=hypothesis, labels=y))\n",
    "\n",
    "train = tf.train.AdamOptimizer(learning_rate=0.001).minimize(cost)\n",
    "\n",
    "predicted = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(predicted, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-30T12:06:52.310561Z",
     "start_time": "2018-12-30T12:05:46.644197Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#4\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "batch_size = 100\n",
    "total_batch = int(mnist.train.num_examples / batch_size)\n",
    "\n",
    "for epoch in range(30):\n",
    "    total_cost = 0\n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)    \n",
    "        c, _ = sess.run([cost, train], feed_dict={x: batch_xs, y: batch_ys, keep_prob:0.5})                \n",
    "        total_cost += c\n",
    "    print(epoch, total_cost)\n",
    "        \n",
    "a = sess.run(accuracy,  feed_dict={x: mnist.test.images, y: mnist.test.labels,keep_prob: 1})\n",
    "print(\"정확도\" ,a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  5. Mnist : SoftMax /  미니배치 / ADAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1\n",
    "learning_rate = 0.01\n",
    "batch_size = 500\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "Y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([784, 10]))\n",
    "b = tf.Variable(tf.random_normal([10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2\n",
    "hypothesis = tf.matmul(X, W) + b\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "    logits=hypothesis, labels=Y))\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for epoch in range(30) :\n",
    "    avg_cost = 0\n",
    "    total_batch = int(mnist.train.num_examples / batch_size)\n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        feed_dict = {X: batch_xs, Y: batch_ys}\n",
    "        c, _ = sess.run([cost, optimizer], feed_dict=feed_dict)\n",
    "        avg_cost += c / total_batch\n",
    "\n",
    "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n",
    "\n",
    "print('Learning Finished!')\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print('Accuracy:', sess.run(accuracy, feed_dict={\n",
    "      X: mnist.test.images, Y: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-30T12:20:51.525660Z",
     "start_time": "2018-12-30T12:20:35.691696Z"
    }
   },
   "outputs": [],
   "source": [
    "#4\n",
    "r = random.randint(0, mnist.test.num_examples - 1)\n",
    "# mnist.test.labels[r] = > vector\n",
    "# mnist.test.labels[r:r+1] = > v matrix\n",
    "\n",
    "a = mnist.test.labels[r] \n",
    "b = mnist.test.labels[r:r+1] \n",
    "\n",
    "print(a.shape)\n",
    "print(b.shape)\n",
    "\n",
    "\n",
    "print(\"Label: \", sess.run(tf.argmax(mnist.test.labels[r:r + 1], 1)))\n",
    "print(\"Prediction: \", sess.run(\n",
    "    tf.argmax(hypothesis, 1), feed_dict={X: mnist.test.images[r:r + 1]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-08T16:39:46.229044Z",
     "start_time": "2018-12-08T16:39:46.224056Z"
    },
    "heading_collapsed": true
   },
   "source": [
    "# 6. Mnist : CNN / dropout / 미니배치 / ADAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#1\n",
    "x = tf.placeholder(\"float\", shape=[None, 784])\n",
    "y = tf.placeholder(\"float\", shape=[None, 10])\n",
    "\n",
    "keep_prob_c = tf.placeholder(tf.float32)\n",
    "keep_prob_h = tf.placeholder(tf.float32)\n",
    "\n",
    "x_image = tf.reshape(x, [-1,28,28,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#2\n",
    "def convolutionLayer(input, nfilter, keep_prob) :\n",
    "    \n",
    "    size = input.get_shape().as_list()        \n",
    "    nhidden = size[3]   # 채널수거나 이전 layer 의 필터수\n",
    "\n",
    "    filter = tf.Variable(tf.truncated_normal([3, 3, nhidden, nfilter], stddev=0.1))\n",
    "    b = tf.Variable(tf.constant(0.01, shape=[nfilter]))\n",
    "    conv = tf.nn.relu(tf.nn.conv2d(input, filter, strides=[1, 1, 1, 1], padding='SAME') + b)\n",
    "    pool = tf.nn.max_pool(conv, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')  \n",
    "    pool_drop = tf.nn.dropout(pool, keep_prob)\n",
    "    \n",
    "    return pool_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#3\n",
    "def fullconnectedLayer(pool, nhidden, keep_prob) :\n",
    "    size = pool.get_shape().as_list()    \n",
    "    flatten =  size[1] *  size[2] * size[3]\n",
    "    \n",
    "    w_fc = tf.Variable(tf.truncated_normal([flatten, nhidden], stddev=0.1))\n",
    "    b_fc = tf.Variable(tf.constant(0.01, shape=[nhidden]))\n",
    "    h_flatten = tf.reshape(pool, [-1, flatten])\n",
    "    h_fc = tf.nn.relu(tf.matmul(h_flatten, w_fc) + b_fc)\n",
    "    h_fc_drop = tf.nn.dropout(h_fc, keep_prob)\n",
    "    return  h_fc_drop    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#4\n",
    "def outputLayer(h_fc_drop, nclass) :    \n",
    "    \n",
    "    size = h_fc_drop.get_shape().as_list()        \n",
    "    nhidden = size[1]\n",
    "    w_fc = tf.Variable(tf.truncated_normal([nhidden, nclass], stddev=0.1))\n",
    "    b_fc = tf.Variable(tf.constant(0.01, shape=[nclass]))\n",
    "    logits = tf.matmul(h_fc_drop, w_fc) + b_fc\n",
    "    return  logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#5\n",
    "h_pool1 = convolutionLayer(x_image, 32, keep_prob_c)\n",
    "h_pool2 = convolutionLayer(h_pool1, 64, keep_prob_c)\n",
    "h_fc1_drop = fullconnectedLayer(h_pool2, 1024, keep_prob_h)\n",
    "logits = outputLayer(h_fc1_drop, 10)\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "train = tf.train.AdamOptimizer(0.001).minimize(cost)\n",
    "\n",
    "\n",
    "prediction = tf.equal(tf.argmax(tf.nn.softmax(logits),1), tf.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(prediction, \"float\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#6\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "batch_size = 100\n",
    "total_batch = int(mnist.train.num_examples / batch_size)\n",
    "\n",
    "total_cost = 0\n",
    "for i in range(total_batch):\n",
    "    batch_xs, batch_ys = mnist.train.next_batch(batch_size)    \n",
    "    _, c = sess.run([train, cost], feed_dict={x: batch_xs, y: batch_ys, keep_prob_c:0.8, keep_prob_h:0.5})                \n",
    "    total_cost += c\n",
    "a = sess.run(accuracy, feed_dict={x: mnist.test.images, y: mnist.test.labels, keep_prob_c: 1.0, keep_prob_h:1.0})\n",
    "print(\"accuracy=%g\"% (a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#7\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "batch_size = 100\n",
    "total_batch = int(mnist.train.num_examples / batch_size)\n",
    "\n",
    "for epoch in range(15):\n",
    "    total_cost = 0\n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)    \n",
    "        _, c = sess.run([train, cost], feed_dict={x: batch_xs, y: batch_ys, keep_prob_c:0.8, keep_prob_h:0.5})                \n",
    "        total_cost += c\n",
    "    a = sess.run(accuracy, feed_dict={x: mnist.test.images, y: mnist.test.labels, keep_prob_c: 1.0, keep_prob_h:1.0})\n",
    "    print(\"%d  cost=%g    accuracy=%g\"% (epoch, total_cost, a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-30T14:43:44.487338Z",
     "start_time": "2018-12-30T14:43:40.529974Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#8 \n",
    "hypothesis = tf.nn.softmax(logits)\n",
    "predicted = tf.argmax(hypothesis, 1)\n",
    "\n",
    "\n",
    "confusion_matrix = tf.confusion_matrix(labels=tf.argmax(y,1), predictions=predicted, num_classes=10)    \n",
    "cm = sess.run(confusion_matrix, feed_dict={x: mnist.test.images, y: mnist.test.labels, keep_prob_c: 1.0, keep_prob_h:1.0})\n",
    "\n",
    "print(cm )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-12T12:11:14.295913Z",
     "start_time": "2018-12-12T12:11:14.291925Z"
    }
   },
   "source": [
    "# 7. Fashion :  SoftMax  /  미니비채 / ADAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "Y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([784, 10]))\n",
    "b = tf.Variable(tf.random_normal([10]))\n",
    "\n",
    "hypothesis = tf.matmul(X, W) + b\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=hypothesis, labels=Y))\n",
    "train = tf.train.AdamOptimizer(0.001).minimize(cost)\n",
    "\n",
    "prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-12T12:32:39.566168Z",
     "start_time": "2018-12-12T12:32:32.844640Z"
    }
   },
   "outputs": [],
   "source": [
    "#2\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "batch_size = 500\n",
    "total_batch = int(fashion.train.num_examples / batch_size)\n",
    "\n",
    "\n",
    "for epoch in range(15):\n",
    "    avg_cost = 0    \n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = fashion.train.next_batch(batch_size)\n",
    "        c, _ = sess.run([cost, train], feed_dict = {X: batch_xs, Y: batch_ys})\n",
    "        avg_cost += c / total_batch\n",
    "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n",
    "\n",
    "\n",
    "print('Accuracy:', sess.run(accuracy, feed_dict={X: fashion.test.images, Y: fashion.test.labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Fashion : 다층신경망 / 미니배치 / ADAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1\n",
    "X = tf.placeholder('float', [None, 784])\n",
    "Y = tf.placeholder('float', [None, 10])\n",
    "\n",
    "def addLayerReLU(x, node) :    \n",
    "    input = x.get_shape().as_list()    \n",
    "    initializer=tf.contrib.layers.variance_scaling_initializer()    \n",
    "    w = tf.Variable(initializer([input[1], node]))\n",
    "    b = tf.Variable(tf.random_normal([node]))\n",
    "    layer = tf.nn.relu(tf.matmul(x, w) + b)    \n",
    "    return layer\n",
    "\n",
    "def addLayer0(x, node) :    \n",
    "    input = x.get_shape().as_list()    \n",
    "    initializer = tf.contrib.layers.xavier_initializer()\n",
    "    w = tf.Variable(initializer([input[1], node]))\n",
    "    b = tf.Variable(tf.random_normal([node]))\n",
    "    layer = tf.matmul(x, w) + b\n",
    "    return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2\n",
    "layer_1 = addLayerReLU(X, 1024)\n",
    "layer_2 = addLayerReLU(layer_1,1024)\n",
    "hypothesis = addLayer0(layer_2, 10)\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=hypothesis, labels=Y))\n",
    "train = tf.train.AdamOptimizer(0.001).minimize(cost) \n",
    "\n",
    "prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(prediction, \"float\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-12T12:35:24.939937Z",
     "start_time": "2018-12-12T12:34:47.355537Z"
    }
   },
   "outputs": [],
   "source": [
    "#3\n",
    "sess.run(tf.initialize_all_variables())\n",
    "\n",
    "batch_size = 100\n",
    "total_batch = int(fashion.train.num_examples/batch_size)\n",
    "\n",
    "for epoch in range(15):\n",
    "    avg_cost = 0.\n",
    "    for step in range(total_batch):\n",
    "        batch_xs, batch_ys = fashion.train.next_batch(batch_size)\n",
    "        c, _ = sess.run([cost, train], feed_dict = {X: batch_xs, Y: batch_ys})\n",
    "        avg_cost += c/total_batch\n",
    "    print(\"Epoch:\", '%04d' %(epoch+1), \"cost=\", \"{:.9f}\".format(avg_cost))\n",
    "\n",
    "\n",
    "print (\"Accuracy:\", sess.run(accuracy, {X: fashion.test.images, Y: fashion.test.labels}))  # 95"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9  Fashion : CNN /  dropout /  미니배치 / ADAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1\n",
    "x = tf.placeholder(\"float\", shape=[None, 784])\n",
    "y = tf.placeholder(\"float\", shape=[None, 10])\n",
    "\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "x_image = tf.reshape(x, [-1,28,28,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2\n",
    "def convolutionLayer(input, nfilter) :\n",
    "    \n",
    "    size = input.get_shape().as_list()        \n",
    "    nhidden = size[3] \n",
    "\n",
    "    w_conv = tf.Variable(tf.truncated_normal([3, 3, nhidden, nfilter], stddev=0.1))\n",
    "    b_conv = tf.Variable(tf.constant(0.01, shape=[nfilter]))\n",
    "    h_conv = tf.nn.relu(tf.nn.conv2d(input, w_conv, strides=[1, 1, 1, 1], padding='SAME') + b_conv)\n",
    "    h_pool = tf.nn.max_pool(h_conv, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')    \n",
    "    return h_pool\n",
    "\n",
    "def fullconnectedLayer(h_pool, nhidden, keep_prob) :\n",
    "    size = h_pool.get_shape().as_list()    \n",
    "    flatten =  size[1] *  size[2] * size[3]\n",
    "    \n",
    "    w_fc = tf.Variable(tf.truncated_normal([flatten, nhidden], stddev=0.1))\n",
    "    b_fc = tf.Variable(tf.constant(0.001, shape=[nhidden]))\n",
    "    h_pool_flat = tf.reshape(h_pool, [-1, flatten])\n",
    "    h_fc = tf.nn.relu(tf.matmul(h_pool_flat, w_fc) + b_fc)\n",
    "    h_fc_drop = tf.nn.dropout(h_fc, keep_prob)\n",
    "    return  h_fc_drop    \n",
    "  \n",
    "def outputLayer(h_fc_drop, nclass) :    \n",
    "    \n",
    "    #h_fc1_drop = [None, 100]\n",
    "    size = h_fc_drop.get_shape().as_list()        \n",
    "    nhidden = size[1]\n",
    "    w_fc = tf.Variable(tf.truncated_normal([nhidden, nclass], stddev=0.1))\n",
    "    b_fc = tf.Variable(tf.constant(0.01, shape=[nclass]))\n",
    "    logits = tf.matmul(h_fc_drop, w_fc) + b_fc\n",
    "    return  logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3\n",
    "h_pool1 = convolutionLayer(x_image, 32)\n",
    "h_pool2 = convolutionLayer(h_pool1, 64)\n",
    "h_fc1_drop = fullconnectedLayer(h_pool2, 1024, keep_prob)\n",
    "logits = outputLayer(h_fc1_drop, 10)\n",
    "\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "train = tf.train.AdamOptimizer(0.001).minimize(cost)\n",
    "\n",
    "\n",
    "prediction = tf.equal(tf.argmax(tf.nn.softmax(logits),1), tf.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(prediction, \"float\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-12T12:48:45.476859Z",
     "start_time": "2018-12-12T12:46:58.709117Z"
    }
   },
   "outputs": [],
   "source": [
    "#4\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "batch_size = 100\n",
    "total_batch = int(fashion.train.num_examples / batch_size)\n",
    "\n",
    "for epoch in range(15):\n",
    "    total_cost = 0\n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = fashion.train.next_batch(batch_size)    \n",
    "        _, c = sess.run([train, cost], feed_dict={x: batch_xs, y: batch_ys, keep_prob:0.5})                \n",
    "        total_cost += c\n",
    "    a = sess.run(accuracy, feed_dict={x: fashion.test.images, y: fashion.test.labels, keep_prob:1.0})\n",
    "    print(\"%d  cost=%g    accuracy=%g\"% (epoch, total_cost, a))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
